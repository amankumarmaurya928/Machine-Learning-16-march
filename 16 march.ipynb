{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d50531a-724c-49f0-adda-45e42e02e03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Underfitting means that your model makes accurate, but initially incorrect predictions. In this case, train error is large\\n     and val/test error is large too. \\n   Overfitting means that your model makes not accurate predictions. In this case, train error\\n     is very small and val/test error is large.\\n\\nIn data science, overfitting simply means that the learning model is far too dependent on training data while underfitting \\nmeans that the model has a poor relationship with the training data. Ideally, both of these should not exist in models, but\\nthey usually are hard to eliminate.   \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1\n",
    "'''Underfitting means that your model makes accurate, but initially incorrect predictions. In this case, train error is large\n",
    "     and val/test error is large too. \n",
    "   Overfitting means that your model makes not accurate predictions. In this case, train error\n",
    "     is very small and val/test error is large.\n",
    "\n",
    "In data science, overfitting simply means that the learning model is far too dependent on training data while underfitting \n",
    "means that the model has a poor relationship with the training data. Ideally, both of these should not exist in models, but\n",
    "they usually are hard to eliminate.   \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac63ca8b-bfb4-440b-b5cd-f44ea6ab632b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can prevent overfitting by diversifying and scaling your training data set or using some other data science strategies, like those given below.\\n1. Early stopping:\\n    Early stopping pauses the training phase before the machine learning model learns the noise in the data. \\n    However, getting the timing right is important; else the model will still not give accurate results.\\n2. Pruning\\n   You might identify several features or parameters that impact the final prediction when you build a model.\\n   Feature selection—or pruning—identifies the most important features within the training set and eliminates irrelevant ones.\\n   example: to predict if an image is an animal or human, you can look at various input parameters like face shape,\\n            ear position, body structure, etc. You may prioritize face shape and ignore the shape of the eyes.\\n3. Regularization:\\n   Regularization is a collection of training/optimization techniques that seek to reduce overfitting. These methods try to \\n   eliminate those factors that do not impact the prediction outcomes by grading features based on importance. \\n   example, mathematical calculations apply a penalty value to features with minimal impact. Consider a statistical model attempting to predict the housing prices of a city in 20 years. Regularization would give a lower penalty value to features like population growth and average annual income but a higher penalty value to the average annual temperature of the city.\\n4. Ensembling:\\n   Ensembling combines predictions from several separate machine learning algorithms. Some models are called weak learners\\n   because their results are often inaccurate. Ensemble methods combine all the weak learners to get more accurate results. \\n   They use multiple models to analyze sample data and pick the most accurate outcomes. The two main ensemble methods are \\n   bagging and boosting. Boosting trains different machine learning models one after another to get the final result, while \\n   bagging trains them in parallel.\\n5. Data augmentation:\\n   Data augmentation is a machine learning technique that changes the sample data slightly every time the model processes it.\\n   You can do this by changing the input data in small ways. When done in moderation, data augmentation makes the training \\n   sets appear unique to the model and prevents the model from learning their characteristics. \\n   example: applying transformations such as translation, flipping, and rotation to input images.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2\n",
    "'''You can prevent overfitting by diversifying and scaling your training data set or using some other data science strategies, like those given below.\n",
    "1. Early stopping:\n",
    "    Early stopping pauses the training phase before the machine learning model learns the noise in the data. \n",
    "    However, getting the timing right is important; else the model will still not give accurate results.\n",
    "2. Pruning\n",
    "   You might identify several features or parameters that impact the final prediction when you build a model.\n",
    "   Feature selection—or pruning—identifies the most important features within the training set and eliminates irrelevant ones.\n",
    "   example: to predict if an image is an animal or human, you can look at various input parameters like face shape,\n",
    "            ear position, body structure, etc. You may prioritize face shape and ignore the shape of the eyes.\n",
    "3. Regularization:\n",
    "   Regularization is a collection of training/optimization techniques that seek to reduce overfitting. These methods try to \n",
    "   eliminate those factors that do not impact the prediction outcomes by grading features based on importance. \n",
    "   example, mathematical calculations apply a penalty value to features with minimal impact. Consider a statistical model attempting to predict the housing prices of a city in 20 years. Regularization would give a lower penalty value to features like population growth and average annual income but a higher penalty value to the average annual temperature of the city.\n",
    "4. Ensembling:\n",
    "   Ensembling combines predictions from several separate machine learning algorithms. Some models are called weak learners\n",
    "   because their results are often inaccurate. Ensemble methods combine all the weak learners to get more accurate results. \n",
    "   They use multiple models to analyze sample data and pick the most accurate outcomes. The two main ensemble methods are \n",
    "   bagging and boosting. Boosting trains different machine learning models one after another to get the final result, while \n",
    "   bagging trains them in parallel.\n",
    "5. Data augmentation:\n",
    "   Data augmentation is a machine learning technique that changes the sample data slightly every time the model processes it.\n",
    "   You can do this by changing the input data in small ways. When done in moderation, data augmentation makes the training \n",
    "   sets appear unique to the model and prevents the model from learning their characteristics. \n",
    "   example: applying transformations such as translation, flipping, and rotation to input images.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c0fb74-59c7-476e-ba00-046661c31382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and \\n   output variables accurately, generating a high error rate on both the training set and unseen data.\\n\\nwhen a model is too simple — informed by too few features or regularized too much — which makes it inflexible in learning from\\nthe dataset. Simple learners tend to have less variance in their predictions but more bias towards wrong outcomes.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3\n",
    "'''Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and \n",
    "   output variables accurately, generating a high error rate on both the training set and unseen data.\n",
    "\n",
    "when a model is too simple — informed by too few features or regularized too much — which makes it inflexible in learning from\n",
    "the dataset. Simple learners tend to have less variance in their predictions but more bias towards wrong outcomes.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c3d90b-c982-4404-9d25-b9a26e391c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the\\n   parameter estimated across samples can be reduced by increasing the bias in the estimated parameters.\\nBias and variance are inversely connected. It is impossible to have an ML model with a low bias and a low variance. When a \\ndata engineer modifies the ML algorithm to better fit a given data set, it will lead to low bias—but it will increase variance.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4\n",
    "'''In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the\n",
    "   parameter estimated across samples can be reduced by increasing the bias in the estimated parameters.\n",
    "Bias and variance are inversely connected. It is impossible to have an ML model with a low bias and a low variance. When a \n",
    "data engineer modifies the ML algorithm to better fit a given data set, it will lead to low bias—but it will increase variance.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab36a45-1be4-42d5-9111-e6b2d56a4260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'common methods for detecting overfitting and underfitting:\\n   We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction \\n   error on the training data and the evaluation data. Your model is underfitting the training data when the model performs \\n   poorly on the training data.\\n   \\n   We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction \\n   error on the training data and the evaluation data. Your model is underfitting the training data when the model performs \\n   poorly on the training data.\\n   '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5\n",
    "'''common methods for detecting overfitting and underfitting:\n",
    "   We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction \n",
    "   error on the training data and the evaluation data. Your model is underfitting the training data when the model performs \n",
    "   poorly on the training data.\n",
    "   \n",
    "   We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction \n",
    "   error on the training data and the evaluation data. Your model is underfitting the training data when the model performs \n",
    "   poorly on the training data.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f578ca7a-b032-4f0a-a5e7-87622a59a38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction\\nerror on the training data and the evaluation data. Your model is underfitting the training data when the model performs \\npoorly on the training data.\\n\\nUnderstanding bias and variance, which have roots in statistics, is essential for data scientists involved in machine \\nlearning. Bias and variance are used in supervised machine learning, in which an algorithm learns from training data or a\\nsample data set of known quantities. The correct balance of bias and variance is vital to building machine-learning algorithms\\nthat create accurate results from their models.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6\n",
    "'''We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction\n",
    "error on the training data and the evaluation data. Your model is underfitting the training data when the model performs \n",
    "poorly on the training data.\n",
    "\n",
    "Understanding bias and variance, which have roots in statistics, is essential for data scientists involved in machine \n",
    "learning. Bias and variance are used in supervised machine learning, in which an algorithm learns from training data or a\n",
    "sample data set of known quantities. The correct balance of bias and variance is vital to building machine-learning algorithms\n",
    "that create accurate results from their models.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07519050-8202-48e4-b0ab-b5b66bbe471b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks\\nthe coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model,\\navoiding the risk of Overfitting.\\n\\nThis makes them more prone to overfitting. Regularization is a technique which makes slight modifications to the learning \\nalgorithm such that the model generalizes better. This in turn improves the model's performance on the unseen data as well.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7\n",
    "''' Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks\n",
    "the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model,\n",
    "avoiding the risk of Overfitting.\n",
    "\n",
    "This makes them more prone to overfitting. Regularization is a technique which makes slight modifications to the learning \n",
    "algorithm such that the model generalizes better. This in turn improves the model's performance on the unseen data as well.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8cd0e1-207a-49bf-adf9-52b3cd093e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
